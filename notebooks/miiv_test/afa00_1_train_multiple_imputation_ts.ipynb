{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "447effeb-1650-4f29-93ad-09b100417273",
   "metadata": {},
   "source": [
    "# (0.1) Train multiple-imputation (MI) models\n",
    "One option to resolve missingness for AFA is to use Multiple Imputation. \n",
    "This notebook trains MI models and saves the multiple imputed datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73390d53-00db-403b-b474-735a5febcf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc2d091-d5fb-4212-b7fd-f94cfb57f50e",
   "metadata": {},
   "source": [
    "## Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b60b58e3-6056-41fe-b9a2-dd7e2d28bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from afa.configurations.utils_ts import specify_default_paths_ts\n",
    "# which dataset to work on \n",
    "dataset_name   = \"miiv_test\"\n",
    "\n",
    "# name for of missingness scenario \n",
    "miss_scenario  = 'MCAR_1'\n",
    "\n",
    "# automatically specify some path locations (change paths manually if needed) \n",
    "paths = specify_default_paths_ts(dataset_name = dataset_name , miss_scenario = miss_scenario) \n",
    "\n",
    "# name for ps_model \n",
    "mi_model_name  = 'mi_simple'\n",
    "\n",
    "# new (where to save the model) \n",
    "mi_model_dir = paths['data_dir']  + 'mi_models' + '/' + mi_model_name + '/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49defcd-c9c2-466e-b156-63275bb006c8",
   "metadata": {},
   "source": [
    "### Define model specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dd5657c-ee90-4f83-921a-557ce99c03e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_model_params = {\n",
    "    'name' : mi_model_name, \n",
    "    'directory' : mi_model_dir,\n",
    "    'base_model_params' : {   'model_type': 'simple_imputer' }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20fdab49-d8eb-4b4e-8cc1-436b9eb35f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'auto'  # 'cuda' or 'cpu'\n",
    "\n",
    "# Config for dataset preparation (torch.DataSet class)\n",
    "dataset_params = {\n",
    "    'missingness_value': 'nan',             # Values given to missing values: int, float or 'nan'\n",
    "    'missingness_rate': (0.3),              # The artificially created missingness for training (MCAR)\n",
    "    'device': device  # 'cuda' or 'cpu'     # Device to train on\n",
    "}\n",
    "\n",
    "# Config for dataloader (torch.DataLoader class)\n",
    "dataloader_params = {\n",
    "    'batch_size': 100,                      # Number of samples per batch\n",
    "    'shuffle': False,                       # Shuffle samples in batch?\n",
    "    # 'prefetch_factor': 1, # increase for speed up, experimental\n",
    "    # 'num_workers': 0,     # set higher for faster throughput, experimental\n",
    "    'drop_last': True                       # Drop last batch, if it has a different batch size\n",
    "}\n",
    "\n",
    "# Config for trainer (pytorch_lightning.Trainer class)\n",
    "trainer_params = {\n",
    "    'max_epochs': 100,                      # number of epochs to train\n",
    "    #'auto_lr_find': False,                  # Find best starting lr, experimental\n",
    "    'fast_dev_run': False,                  # Fast dev run to test set up before commencing training\n",
    "    'accelerator': device,                  # device to train on, should be the same as for dataset_params\n",
    "    'devices': 1,                           # Number of devices to train on, leave it at one\n",
    "    'profiler': None,                       # Pytorch profiler, 'simple', 'advanced', None\n",
    "    'num_sanity_val_steps': 0,              # Number of sanity validation steps, for debugging\n",
    "    'wandb_logger': False,                   # Wether to use wandb logger, else Tensorboard is used\n",
    "    #'wandb_project_name': 'GPImputer Synthetic 2'   # Project name, in case wandb logging is used\n",
    "}\n",
    "\n",
    "# Config for gp_model (GPImputer class)\n",
    "gp_params = {\n",
    "    'model_type': 'gaussian_process',       # IMPORTANT: for BaseModelImputer_ts to choose the correct class\n",
    "    'dataset_params' : dataset_params,\n",
    "    'dataloader_params' : dataloader_params,\n",
    "    'trainer_params' : trainer_params,\n",
    "    'num_tasks': 49,  # number of tasks == number of features\n",
    "    'num_kernels': 10,\n",
    "    'data_mode': 'no_simulation',   # 'no_simulation' or 'simulation', with simulation a ground truth is expected to passed as well, ground truth = values for data that is missing in train dataloader, experimental -> leave it as no_simulation\n",
    "    'ckpt_path': None #'best_model-v_recon_loss_target=1.10-epoch=142.ckpt',  # path to checkpoint of trained model, full path or relative to model directory    \n",
    "}\n",
    "    \n",
    "# Config for mi_model from AFA module (MultipleImputationModel_ts class)\n",
    "mi_model_params = {\n",
    "    'name' : mi_model_name, \n",
    "    'directory' : mi_model_dir,\n",
    "    'base_model_params' : gp_params\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a64631a-d0c5-437d-82f9-2427a44f2b27",
   "metadata": {},
   "source": [
    "## Load dataset with missingness \n",
    "At first, we want to load the dataset \n",
    "\n",
    "Includes loading: \n",
    "- superfeature mapping\n",
    "- problem\n",
    "- afa_problem \n",
    "- missingness_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a59d767-871f-4f22-b635-f2af3e832341",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-07 14:43:18.645851: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-07 14:43:18.737386: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-08-07 14:43:18.737403: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-08-07 14:43:22.617555: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-07 14:43:22.617671: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-07 14:43:22.617682: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from afa.data_modelling.datasets.data_loader.data_loader_ts import DataLoader_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7807d31-a689-4abd-a656-7c4f1f147506",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Padding sequences: 100%|██████████| 100/100 [00:00<00:00, 1973.54it/s]\n",
      "Padding sequences: 100%|██████████| 100/100 [00:00<00:00, 2285.23it/s]\n",
      "Padding sequences: 100%|██████████| 100/100 [00:00<00:00, 1665.85it/s]\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader_ts(     data_file                  = paths['data_file'],\n",
    "                                 temporal_data_file         = paths['temporal_data_file'],\n",
    "                                 superfeature_mapping_file  = paths['superfeature_mapping_file'],\n",
    "                                 problem_file               = paths['problem_file'],\n",
    "                                 afa_problem_files          = paths['afa_problem_files'], \n",
    "                                 miss_model_files           = paths['miss_model_files'], \n",
    "                                 folds_file                 = paths['folds_file'] )\n",
    "dataset = data_loader.load() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ffd0f4-07fd-4e6d-a2cb-74ea1eea2c58",
   "metadata": {},
   "source": [
    "## Define MI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "912572d6-2218-419e-b752-b7eadbb18a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128,\n",
      " 'ckpt_path': None,\n",
      " 'data_missingness': 0.6,\n",
      " 'data_mode': 'no_simulation',\n",
      " 'dataloader_params': {'batch_size': 100, 'drop_last': True, 'shuffle': False},\n",
      " 'dataset_name': 'toydataset_50000',\n",
      " 'dataset_params': {'device': 'auto',\n",
      "                    'missingness_rate': 0.3,\n",
      "                    'missingness_value': 'nan'},\n",
      " 'directory': '../../../data/ts/miiv_test/MCAR_1/mi_models/mi_simple/',\n",
      " 'lr': 0.01,\n",
      " 'model_type': 'gaussian_process',\n",
      " 'model_weights_save_path': './model_weights',\n",
      " 'num_epochs': 10,\n",
      " 'num_kernels': 10,\n",
      " 'num_tasks': 49,\n",
      " 'rank': 4,\n",
      " 'sample_tp': 0.4,\n",
      " 'sample_tp_interval': [0.3, 0.8],\n",
      " 'task_names': ['Noise', 'Trend', 'Seasonality', 'Trend + Seasonality'],\n",
      " 'trainer_params': {'accelerator': 'auto',\n",
      "                    'devices': 1,\n",
      "                    'fast_dev_run': False,\n",
      "                    'max_epochs': 100,\n",
      "                    'num_sanity_val_steps': 0,\n",
      "                    'profiler': None,\n",
      "                    'wandb_logger': False}}\n",
      "Created the save folder for the model weights: ../../../data/ts/miiv_test/MCAR_1/mi_models/mi_simple/model_weights\n",
      "No previously trained model found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from afa.data_modelling.missingness.multiple_imputation.multiple_imputation_model_ts import MultipleImputationModel_ts\n",
    "\n",
    "mi_model = MultipleImputationModel_ts(  name                         = mi_model_params['name'], \n",
    "                                        m_graph                      = dataset.miss_model.m_graph, \n",
    "                                        superfeature_mapping         = dataset.superfeature_mapping,\n",
    "                                        target_superfeature_names    = dataset.afa_problem.target_superfeature_names,\n",
    "                                        model_params                 = mi_model_params,\n",
    "                                        directory                    = mi_model_params['directory'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eaaaf5-3403-46c3-8bf9-7dffd270853b",
   "metadata": {},
   "source": [
    "## Train MI model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cee7ae1-f3e9-4375-9551-ce7921214634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unloading trained model and reinstantiating new model and trainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                       | Params\n",
      "--------------------------------------------------------\n",
      "0 | mae_loss | L1Loss                     | 0     \n",
      "1 | mgp      | HadamardGP                 | 2.6 K \n",
      "2 | mll      | ExactMarginalLogLikelihood | 2.6 K \n",
      "--------------------------------------------------------\n",
      "2.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.6 K     Total params\n",
      "0.010     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93676c269ca348039e73329c96b376ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: No training batches.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit finished.\n",
      "{'batch_size': 128,\n",
      " 'ckpt_path': None,\n",
      " 'data_missingness': 0.6,\n",
      " 'data_mode': 'no_simulation',\n",
      " 'dataloader_params': {'batch_size': 100, 'drop_last': True, 'shuffle': False},\n",
      " 'dataset_name': 'toydataset_50000',\n",
      " 'dataset_params': {'device': 'auto',\n",
      "                    'missingness_rate': 0.0,\n",
      "                    'missingness_value': 'nan'},\n",
      " 'directory': '../../../data/ts/miiv_test/MCAR_1/mi_models/mi_simple/',\n",
      " 'lr': 0.01,\n",
      " 'model_type': 'gaussian_process',\n",
      " 'model_weights_save_path': './model_weights',\n",
      " 'num_epochs': 10,\n",
      " 'num_kernels': 10,\n",
      " 'num_tasks': 49,\n",
      " 'rank': 4,\n",
      " 'sample_tp': 0.4,\n",
      " 'sample_tp_interval': [0.3, 0.8],\n",
      " 'task_names': ['Noise', 'Trend', 'Seasonality', 'Trend + Seasonality'],\n",
      " 'trainer_params': {'accelerator': 'auto',\n",
      "                    'devices': 1,\n",
      "                    'fast_dev_run': False,\n",
      "                    'max_epochs': 100,\n",
      "                    'num_sanity_val_steps': 0,\n",
      "                    'profiler': None,\n",
      "                    'wandb_logger': False}}\n",
      "Performance plot currently not implemented\n"
     ]
    }
   ],
   "source": [
    "mi_model.fit(dataset, fold = 0, train_split = 'train', valid_split = 'val', fit_again = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547a323f-b345-4a74-90aa-5ea170b0451e",
   "metadata": {},
   "source": [
    "## Create multiple imputed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2fc5fbc-6787-45f6-8832-0e7a9cebad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from afa.data_modelling.missingness.multiple_imputation.multiple_imputed_dataset_ts import MultipleImputedDataset_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6afc7a63-5409-4327-b48e-829d285411c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732843c002dd41c1957fc59a0591fd78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch sampling:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mi_results = mi_model.predict(dataset, n_samples = 5)\n",
    "\n",
    "# create an mi_dataset out of the generated imputations\n",
    "mi_dataset = MultipleImputedDataset_ts(  dataset = dataset, model = mi_model, results = mi_results) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c90c49-db33-426f-81c0-d53e70986225",
   "metadata": {},
   "source": [
    "## Evaluate imputation model on ground truth dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7c58a8-7b09-4174-8e78-1c3be89a5c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8c9f03-5639-4f5f-baef-36cbea96789b",
   "metadata": {},
   "source": [
    "## Save MI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37204a6c-b06f-4ff0-8559-f6977ecf1cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_dataset.save( model_dir = mi_model_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cb32c7-1f0b-4f16-9fd4-987260ffed06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ef49f9-dbde-4665-9ad8-c3eec083b86a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "afa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
